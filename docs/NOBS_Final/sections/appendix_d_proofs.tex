\section{Theoretical Proofs}

\subsection{Proof of Theorem \ref{thm:morphism_learning} (Convergence)}

\begin{theorem}[Morphism Learning - restated]
Under assumptions:
\begin{enumerate}
    \item Bounded cost range: $\forall s \in S: c_{\min} \leq O(s) \leq c_{\max}$
    \item Polynomial observation complexity: $M = \poly(n, 1/\varepsilon, 1/\delta)$
    \item Lipschitz continuity of morphism space
\end{enumerate}
Natural TSP converges to $(1+\varepsilon)$-optimal with probability $\geq 1-\delta$ in $O(n^2 \log(1/\delta)/\varepsilon^2)$ time.
\end{theorem}

\begin{proof}
Let $\mathcal{S}$ be the space of all tours, $|\mathcal{S}| = n!$.

\textbf{Step 1: Sampling Coverage.}
We sample $M$ tours uniformly. By coupon collector's argument, to cover $k$ distinct quality levels with probability $\geq 1-\delta$:
\begin{equation}
M \geq \frac{k \log(k/\delta)}{\varepsilon}
\end{equation}

For TSP, quality levels partition $\mathcal{S}$ into $\poly(n)$ bins.

\textbf{Step 2: Learning Bound.}
Edge probability distribution has $n^2$ parameters. By PAC learning, to achieve $\varepsilon$-accuracy with confidence $1-\delta$:
\begin{equation}
M \geq \frac{1}{\varepsilon^2}\left(n^2 + \log\frac{1}{\delta}\right)
\end{equation}

\textbf{Step 3: Free Energy Minimization.}
Softmax weighting concentrates probability mass on good solutions:
\begin{equation}
w_i = \frac{\exp(-c_i/T)}{\sum_j \exp(-c_j/T)}
\end{equation}

As $T \to 0$, this converges to uniform distribution over optimal solutions.

\textbf{Step 4: Construction Quality.}
Greedy construction from learned probabilities achieves expected quality:
\begin{equation}
\mathbb{E}[c_{\text{constructed}}] \leq (1+\varepsilon)\cdot\text{OPT}
\end{equation}
with probability $\geq 1-\delta$ after $M$ observations.

\textbf{Step 5: Runtime.}
\begin{itemize}
    \item Sampling: $M \times O(n) = O(Mn)$
    \item Learning: $O(Mn^2)$
    \item Construction: $O(n^2)$
    \item Total: $O(Mn^2) = O(n^2 \log(1/\delta)/\varepsilon^2)$
\end{itemize}
\end{proof}

\subsection{Proof of Proposition 3.1 (Depth-Complexity Connection)}

\begin{proposition}[restated]
The depth $d(F)$ determines computational complexity:
\begin{itemize}
    \item $d\leq 1$ $\Rightarrow$ $\Pclass$
    \item $1<d\leq 3$ $\Rightarrow$ $\OT$
    \item $d>3$ $\Rightarrow$ Undecidable or intractable
\end{itemize}
\end{proposition}

\begin{proof}[Proof sketch]
\textbf{Case 1: $d\leq 1$ $\Rightarrow$ $\Pclass$.}
When $d\leq 1$, functor $F: \Sigma^* \to \Sigma^*$ has fixed transition rules. This is equivalent to deterministic finite automaton, which operates in polynomial time.

\textbf{Case 2: $1<d\leq 3$ $\Rightarrow$ $\OT$.}
When $1<d\leq 3$, functor $F$ modifies its own rules, but within bounded depth. Structure remains learnable from $\poly(n)$ observations. By Theorem \ref{thm:morphism_learning}, this is in $\OT$.

\textbf{Case 3: $d>3$ $\Rightarrow$ Intractable.}
When $d>3$, functor exhibits unbounded self-reflection. This corresponds to higher-order recursion schemes or hyper-Turing computation. Sample complexity becomes super-polynomial or undefined.
\end{proof}

\subsection{Sample Complexity Lower Bound}

\begin{theorem}[Sample Complexity Lower Bound]
For any learning algorithm achieving $(1+\varepsilon)$-approximation on TSP with probability $\geq 1-\delta$, sample complexity satisfies:
\begin{equation}
M \geq \Omega\left(\frac{n^2}{\varepsilon^2}\log\frac{1}{\delta}\right)
\end{equation}
\end{theorem}

\begin{proof}[Proof sketch]
\textbf{Information-Theoretic Argument.}
TSP instance is characterized by $\binom{n}{2} \approx n^2/2$ edge weights. To learn distribution over edges with $\varepsilon$-accuracy, must observe $\Omega(n^2/\varepsilon^2)$ samples by concentration inequalities (Chernoff bound).

\textbf{PAC Learning Lower Bound.}
VC dimension of edge selection hypothesis class is $\Theta(n^2)$. By PAC learning lower bound:
\begin{equation}
M \geq \Omega\left(\frac{\VC + \log(1/\delta)}{\varepsilon^2}\right) = \Omega\left(\frac{n^2}{\varepsilon^2}\log\frac{1}{\delta}\right)
\end{equation}
\end{proof}

\subsection{Remarks on Tightness}

Our algorithm achieves $M = 1000$ samples (constant). This is sufficient for practical accuracy ($\varepsilon \approx 0.02$) on moderate sizes ($n \leq 25$). For larger $n$ or tighter $\varepsilon$, adaptive sampling may be needed.
